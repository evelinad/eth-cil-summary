\documentclass[a4paper, 11pt, landscape]{article}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{hyperref} % for Todo's and similar things
\usepackage[margin=0.5cm, landscape]{geometry}
\usepackage[small,compact]{titlesec}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

% compact text
\linespread{0.95}
\addtolength{\parindent}{-6mm}

% compact lists
\setlist[description]{leftmargin=*, labelwidth=0em, itemindent=0em, itemsep=0em, parsep=0em, nolistsep}
\setlist[itemize]{leftmargin=*, labelwidth=0em, itemindent=0em, itemsep=0em, parsep=0em, nolistsep}
\setlist[enumerate]{leftmargin=*, labelwidth=0em, itemindent=0em, itemsep=0em, parsep=0em, nolistsep}

% compact sections
\titlespacing*{\section}{0pt}{0em}{0em}
\titlespacing*{\subsection}{0pt}{0em}{0em}
\titlespacing*{\subsubsection}{0pt}{0em}{0em}

% coloured section headings for easier read
\titleformat{name=\section}[block]
  {\sffamily}
  {}
  {0pt}
  {\colorsection}
\newcommand{\colorsection}[1]{%
	\colorbox{blue!10}{\parbox[t][0em]{\dimexpr\columnwidth-2\fboxsep}{\thesection\ #1}}}


\titleformat{name=\subsection}[block]
  {\sffamily}
  {}
  {0pt}
  {\subcolorsection}
\newcommand{\subcolorsection}[1]{%
	\colorbox{orange!10}{\parbox[t][0em]{\dimexpr\columnwidth-2\fboxsep}{\thesubsection\ #1}}}

% multicols lines & spacing
\setlength{\columnsep}{0.2cm}
\setlength{\columnseprule}{0.2pt}

% No page numbers
\pagenumbering{gobble}

% math helpers
\newcommand{\matr}[1]{\boldsymbol{\mathrm{#1}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
\begin{multicols*}{3}

\section{PCA}
$\matr{X} \in \mathbb{R}^{D \times N}$. $N$ observations, $K$ properties. Target: $\tilde{\matr{X}} \in \mathbb{R}^{K \times N}$.
\begin{enumerate}
	\item Empirical Mean: $\overline{\matr{x}} = \frac{1}{N} \sum_{n=1}^N \matr{x}_n$
	\item Center Data: $\overline{\matr{X}} = \matr{X} - [\overline{\matr{x}}, \ldots, \overline{\matr{x}}] = \matr{X} - \matr{M}$
	\item Covariance Matrix: $\matr{\Sigma} = \frac{1}{N	} \sum_{n=1}^N (\matr{x}_n - \overline{\matr{x}}) (\matr{x}_n - \overline{\matr{x}})^T = \frac{1}{N} \overline{\matr{X}}\overline{\matr{X}}^T$
	\item Eigenvalue Decomposition: $\matr{\Sigma} = \matr{U \Lambda U}^T$, sort eigenvalues (and eigenvectors) in descending order
	\item Select $K < D$, keep only the first $K$ eigenvalues and corresponding eigenvectors $\Rightarrow \matr{U}_K, \matr{\Lambda}_K$
	\item Transform data onto new Basis: $\overline{\matr{Z}} = \matr{U}_K^T \overline{\matr{X}}$
	\item Reconstruct to original Basis: $\tilde{\overline{\matr{X}}} = \matr{U}_k \overline{\matr{Z}}$
	\item Reverse centring: $\tilde{\matr{X}} = \tilde{\overline{\matr{X}}} + \matr{M}$
\end{enumerate}

\begin{description}
	\item[Approx. Error:] Sum of discarded eigenvalues
\end{description}

\begin{itemize}
	\item TODO: Covariance Matrix of the result is diagonlized
	\item TODO: Properties needed to be known?
\end{itemize}

\section{Eigenvalue / -vectors}
Eigenvalue Problem: $\matr{Ax} = \lambda \matr{x}$
\begin{enumerate}
	\item solve $(\matr{A} - \lambda \matr{I}) \matr{x} = \matr{0}$ for all $\matr{x} \neq \matr{0}$, done by solving $\operatorname{det}(\matr{A} - \lambda \matr{I}) \overset{!}{=} 0$
	\item for each found $\lambda_i$:
		solve $(\matr{A} - \lambda_i \matr{I}) \matr{x}_i = \matr{0}$, $\matr{x}_i$ is the $i$-th eigenvector.
	\item (opt.) normalize eigenvector $q_i$: $q_i^{\text{norm}} = \frac{1}{\|q_i\|_2} q_i$.
\end{enumerate}

\section{Eigendecomposition}
$\matr{A} \in \mathbb{R}^{N \times N}$ with $N$ linear independent eigenvectors, then: $\matr{A} = \matr{Q \Lambda Q}^{-1}$. $\matr{Q} \in \mathbb{R}^{N \times N}$, $i$-th column is the $i$-th eigenvector of $\matr{A}$. $\matr{\Lambda}$ is diagonal and $\matr{\Lambda}_{i,i}$ is the $i$-th eigenvector. Every symmetric matrix can be ``eigendecomposed''.

$\matr{A}^{-1} = \matr{Q} \matr{\Lambda}^{-1} \matr{Q}^{-1}$, $\left[ \matr{\Lambda}^{-1} \right]_{i,i} = \frac{1}{\lambda_i}$

Calculate eigenvalues $\lambda_i$ and eigenvectors $q_i$. Normalise the eigenvectors. Put eigenvectors as columns into $\matr{Q}$ and eigenvalues into the diagonal of $\matr{\Lambda}$.

\section{SVD}
\begin{itemize}
	\item $\matr{A} = \matr{U} \matr{D} \matr{V}^T = \sum_{k=1}^{\operatorname{rank}(\matr{A})} d_{k,k} u_k v_k^T$
	\item $\matr{A} \in \mathbb{R}^{N \times P}, \matr{U} \in \mathbb{R}^{N \times N}, \matr{D} \in \mathbb{R}^{N \times P}, \matr{V} \in \mathbb{R}^{P \times P}$
	\item $\matr{U}^T \matr{U} = I = \matr{V}^T \matr{V}$ ($\matr{U}, \matr{V}$ columns are orthonormal)
	\item $\matr{U}$ columns are eigenvectors of $\matr{A} \matr{A}^T$, $\matr{V}$ columns are eigenvectors of $\matr{A}^T \matr{A}$, $\matr{D}$ diagonal elements are singular values, i.e. the square roots of the eigenvalues ($\matr{A}^T \matr{A}$ and $\matr{A} \matr{A}^T$ have the same eigenvalues)
	\item $\matr{A}^T = \matr{V} \matr{D} \matr{U}^T$
	\item $\matr{A}^T \matr{A} = \matr{A} \matr{D}^2 \matr{V}^T$
	\item $(\matr{D}^{-1})_{i,i} = \frac{1}{\matr{D}_{i, i}}$ ($\matr{D} \in \mathbb{R}^{N \times P} \to \matr{D}^{-1} \in \mathbb{R}^{P \times N}$, i.e. don't forget to transpose)
\end{itemize}

\begin{enumerate}
	\item calculate $\matr{A}^T \matr{A}$.
	\item calculate eigenvalues of $\matr{A}^T \matr{A}$, the square root of them are the diagonal elements of $\matr{D}$.
	\item calculate eigenvectors of $\matr{A}^T \matr{A}$ using the eigenvalues resulting in the columns of $\matr{V}$.
	\item calculate the missing matrix: $\matr{U} = \matr{A} \matr{V} \matr{D}^{-1}$. Can be checked by calculating the eigenvectors of $\matr{A} \matr{A}^T$.
	\item normalize each columns of $\matr{U}$ and $\matr{V}$.
\end{enumerate}

\subsection{Low-Rank approximation}
Using only $K$ largest eigenvalues and corresponding eigenvectors.
\begin{description}
	\item[Approx. Error Frobenius Norm:] $\|\matr{A} - \tilde{\matr{A}}\|_F = \sqrt{\sum_{i > K} \sigma_i^2} = \sqrt{\sum_{i > K} \lambda_i}$
	\item[Approx. Error Euclidean Norm:] $\|\matr{A} - \tilde{\matr{A}}\|_2 = \sigma_{K+1}$ (i.e. largest singular value that was thrown away)
\end{description}

\section{Basis Transform}
Let $\matr{V}, \matr{W}$ be two matrices representing two basis (each column is a basis vector and they are invertible).

The given vector $\matr{x}$ is represented by the \emph{coordinate vector} $\matr{a}$ in basis $\matr{V}$: $\matr{x} = \matr{V} \matr{a} \Rightarrow \matr{a} = \matr{V}^{-1} \matr{x}$. In other words: if we have a basis $\matr{V}$ and the vector $\matr{x}$ we can get the vector's representation in the basis $\matr{V}$ by calculating $\matr{a} = \matr{V}^{-1} \matr{x}$.

Remember $\matr{x}$ here is just a vector with values and is not based on any basis. Just an array of values! By calculating $\matr{a}$ we get a vector that represents these values based on a basis!

\section{Matrix/Vector}
\begin{description}
	\item[Symmetric:] $\matr{A} = \matr{A}^T$
	\item[Orthonormal:] $\matr{A}^{-1} = \matr{A}^T$, $\matr{A} \matr{A}^T = \matr{A}^T \matr{A} = \matr{I}$, $\operatorname{det}(\matr{A}) \in \{+1, -1\}$, $\operatorname{det}(\matr{A}^T \matr{A}) = 1$
	\item[Inner Product:] (assuming everything in $\mathbb{R}^D$) $\left\langle \matr{x}, \matr{y} \right\rangle = \matr{x}^T \matr{y} = \sum_{i=1}^{N} \matr{x}_i \matr{y}_i$
	\begin{itemize}
		\item $\left\langle \matr{x} \pm \matr{y}, \matr{x} \pm \matr{y} \right\rangle = \left\langle \matr{x}, \matr{x} \right\rangle \pm 2 \left\langle \matr{x}, \matr{y} \right\rangle + \left\langle \matr{y}, \matr{y} \right\rangle$
		\item $\left\langle \matr{x}, \matr{y} + \matr{z} \right\rangle = \left\langle \matr{x}, \matr{y} \right\rangle + \left\langle \matr{x}, \matr{z} \right\rangle$
		\item $\left\langle \matr{x} + \matr{y}, \matr{z} \right\rangle = \left\langle \matr{x}, \matr{z} \right\rangle + \left\langle \matr{y}, \matr{z} \right\rangle$
		\item $\left\langle \matr{x}, \matr{y} + \matr{z} \right\rangle = \left\langle \matr{x}, \matr{y} \right\rangle + \left\langle \matr{x}, \matr{z} \right\rangle$
	\end{itemize}
	\item[Outer Product:] $\matr{u} \matr{v}^T$, $(\matr{u} \matr{v}^T)_{i, j} = \matr{u}_i \matr{v}_j$
	\item[Transpose:]\hfill
		\begin{itemize}
			\item $(\matr{A} + \matr{B})^T = \matr{A}^T + \matr{B}^T$
			\item $(\matr{A}\matr{B})^T = \matr{B}^T \matr{A}^T$
			\item $(\alpha \matr{A})^T = \alpha \matr{A}^T$
			\item $(\matr{A}^T)^{-1} = (\matr{A}^{-1})^T$
		\end{itemize}
\end{description}

\section{Norms}
\begin{description}
	\item[Euclidean:] $\|\matr{x}\|_2 := \sqrt{\sum_{i=1}^{N} \matr{x}_i^2} = \sqrt{\matr{x}^T \matr{x}}$
	\item[$p$-norm:] $\|\matr{x}\|_p := \left( \sum_{i=1}^{N} |x_i|^p \right)^{\frac{1}{p}}$
	\item[Frobenius:] $\|\matr{A}\|_F :=\allowbreak \sqrt{\sum_{i=1}^{M} \sum_{j=1}^{N} |\matr{A}_{i, j}|^2} =\allowbreak \sqrt{\operatorname{trace}(\matr{A}^T \matr{A})} =\allowbreak \sqrt{\sum_{i=1}^{\min\{m, n\}} \sigma_i^2}$ ($\sigma_i$ is the $i$-th singularvalue), $\matr{A} \in \mathbb{R}^{M \times N}$
\end{description}

\section{$K$-means Algorithm}
\begin{enumerate}
	\item Initiate: choose randomly $K$ centroids $\matr{U} = [\matr{u}_1, \ldots, \matr{u}_K]$ (usually $K$ randomly chosen data points).
	\item Assign data points to clusters. $k^\star(\matr{x}_n) = \argmin_k \{ \|\matr{x}_n - \matr{u}_1\|_2^2, \ldots, \|\matr{x}_n - \matr{u}_K\|_2^2 \}$ gives us the cluster $k$ whose centroid $\matr{u}_k$ is closest to data point $\matr{x}_n$. Set accordingly the $k$-th element of the $n$-th column of the assignment matrix $\matr{Z}$ to $1$, all others to $0$.
	\item Update centroids: $\matr{u}_k = \frac{\sum_{n=1}^N z_{k,n} \matr{x}_n}{\sum_{n=1}^N z_{k,n}}$ (each centroid is in the middle of all data points assigned to it's cluster).
	\item Repeat from step 2.
\end{enumerate}
Iteration stops if the half of the number of changes in $\matr{Z}$ is zero ($\frac{1}{2} \|\matr{Z} - \matr{Z}^\text{new}\|_0$).

\section{Probability / Statistics}
\begin{itemize}
	\item $P(x) := Pr[X = x] := \sum_{y \in Y} P(x, y)$
	\item $P(x|y) := Pr[X = x | Y = y] := \frac{P(x,y)}{P(y)},\quad \text{if } P(y) > 0$
	\item $\forall y \in Y: \sum_{x \in X} P(x|y) = 1$ (property for any fixed $y$)
	\item $P(x, y) = P(x|y) P(y)$
	\item $\sum_{y \in Y} P(x, y) = P(x)$
	\item $P(x|y) = \frac{P(y|x)P(x)}{P(y)}$ (Bayes' rule)
	\item Two random variables $X$, $Y$ are called \emph{independent}, if knowing one of them reveal any information about the other one: $P(x|y) = P(x) \Leftrightarrow P(y|x) = P(y)$
	\item \emph{IID} (Independent and Identically Distributed): $n$ random variables are IID if each has the same distribution and they are mutually independent.
	\begin{itemize}
		\item $P(x_1, \ldots, x_n) = \prod_{i=1}^n P(x_i)$
	\end{itemize}
\end{itemize}

\section{Gaussian Mixture Models (GMM)}
\begin{description}
	\item[] $p(\matr{x}) = \sum_{k=1}^K \pi_k \mathcal{N}(\matr{x} | \matr{\mu}_k, \Sigma_k)$
	\item[Likelihood of observed data $\matr{X}$:] $p(\matr{X} | \matr{\pi}, \matr{\mu}, \matr{\Sigma}) = \prod_{n=1}^N p(\matr{x}_n) = \prod_{n=1}^N \sum_{k=1}^N \pi_k \mathcal{N}(\matr{x} | \matr{\mu}_k, \matr{\Sigma}_k)$
	\item[Log-likelihood:] $\log p(\matr{X} | \matr{\pi}, \matr{\mu}, \matr{\Sigma}) =\break \sum_{i=1}^N \log \left( \sum_{k=1}^K \pi_k \mathcal{N}(\matr{x}_i | \matr{\mu}_k, \matr{\Sigma}_k) \right)$
\end{description}

\subsection{Expectation-Maximization (EM)}
\begin{itemize}
	\item Expectation step: Assign data to parts of the model (e.g. assign data point to a mixture component)
	\item Maximization step: Improve the model parameters based on the new assignment
	\item Iterative process, i.e. repeat until converges
	\item may not result in global maximum/optimum
	\item TODO
\end{itemize}

\subsection{Haar Wavelet}
\begin{description}
	\item[Mother function:] $\psi(t)$ is $1$ for $0 \leq t < \frac{1}{2}$, $-1$ for $\frac{1}{2} \leq t < 1$, $0$ otherwise.
	\item[Haar function:] $\psi_{n,k}(t) = 2^{n/2} \psi(2^n t - k)$, $2^{n/2}$ only needed if normalized values needed!
\end{description}

For the not normalized basis: $\matr{W}^{-1} = (\matr{W^T} \matr{W})^{-1} \matr{W}^T = \operatorname{diag}(\frac{1}{l_1}, \ldots, \frac{1}{l_n}) \matr{W}^T$ where $l_i$ is the norm of the $i$-th column of $\matr{W}$. This is valid because $\matr{W}$ is orthogonal!

Don't forget that $[1, 1, \ldots, 1]^T$ is the first non-normalized Haar Wavelet basis!

\section{Optimization}
\subsection{Convex Functions}
A function $f(x)$ is convex if domain of $f$ (set of input values) is convex (e.g. $R^D$) and $\forall y, z, \forall t \in [0, 1]: f(t \cdot y + (1-t) \cdot z) \leq t \cdot f(y) + (1-t) \cdot f(z)$.
\subsection{Gradient Descent}
TODO

\subsection{Stochastic Gradient Descent}
TODO

\subsection{Lagrangian Multipliers}
Minimize  $f(\matr{x})$ with subject to $g_i(\matr{x}) \leq 0,\ i = 1, \ldots, m$ and $h_i(\matr{x}) = \matr{a}_i^T \matr{x} - b_i = 0,\ i = 1, \ldots, p$.
\begin{description}
	\item[Lagrangian:] $L(\matr{x}, \matr{\lambda}, \matr{\nu}) := f(\matr{x}) + \sum_{i=1}^m \lambda_i g_i(\matr{x}) + \sum_{i=1}^p \nu_i h_i(\matr{x})$
	\item[Dual function:] $d(\matr{\lambda}, \matr{\nu}) := \inf_{\matr{x}} L(\matr{x}, \matr{\lambda}, \matr{\nu}) \in \mathbb{R}$
\end{description}
To find the best lower bound on the optimum $f(\matr{x}^\star)$ the dual problem $d(\matr{\lambda}, \matr{\nu})$ is maximized with subject to $\matr{\lambda} \geq \matr{0}$.
\raggedcolumns

\section{TODOs}
\url{https://en.wikipedia.org/wiki/Transpose#Special_transpose_matrices}

\end{multicols*}
\end{document}
