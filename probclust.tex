\chapter{Mixture Models and the Expectation Maximization Algorithm}

\section{Probabilistic Clustering}
Instead of a ``hard'' assignment of data points to a specific cluster (e.g. \(K\)-means algorithm) we now do a probabilistic assignment. Therefore every data point \(\matr{x}_i\) is assigned to each cluster \(k\) with a probability \(z_{k, i}\). Therefore our assignment matrix \(\matr{Z}\) is now constrained by
\begin{itemize}
\item \(\forall k, i: z_{k, i} \in [0, 1]\)
\item \(\sum_{k=1}^K z_{k, i} = 1 \quad \forall n\)
\end{itemize}

As a consequence the objective \(J\) can no longer be interpreted as an approximation matrix factorization problem!

\subsection{Generative Model}
\todo{still not 100\% sure I got the idea. \url{http://cs229.stanford.edu/materials.html} may be a source for better understanding}
A generative model models how data is generated. For a given data it checks which of the known generation models most likely could generate the given data.

Such a model is parametrized by a set of probability distributions \(\mathcal{P} = \{p_\theta : \theta \in \Theta\}\) with
\begin{itemize}
\item \(Pr_\theta[\matr{x} \in A] = \int_A p_\theta(\matr{x}) d\matr{x}\) for measurable \(A\)
\item \(\Theta\) is the space of possible parameter values (e.g. \(\subseteq \R^M\))
\end{itemize}

Now the question remains how to select the optimal parameter \(\Theta\).

\begin{definition}[Likelihood Function]
Probability of observed data under \(\Theta\) is turned into a \textit{likelihood function} for \(\Theta\) given an outcome.

\begin{itemize}
\item Probability = parameter given, events (data) to be generated
\item Likelihood = outcome (data) given, parameter to be inferred
\end{itemize}

\[
\mathcal{L}(\theta;\matr{X}) := p_\theta(\matr{X}) \overset{\ast}{=} \prod_{n=1}^N p_\theta (\matr{x}_n) \quad \ast \text{IID sampling}
\]
\end{definition}

\begin{definition}[Maximum Likelihood Estimator (MLE)]
\[
\hat{\theta} = \argmax_{\theta \in \Theta} p_\theta (\matr{X}) \overset{\ast}{=} \argmax_{\theta \in \Theta} \sum_{n=1}^N \ln p_\theta (\matr{x}_n)
 \quad \ast \text{IID sampling}
\]
\end{definition}

\section{Mixture Models}
\begin{definition}
For a finite mixture model we define
\[
p_\theta = \sum_{k=1}^K \pi_k p(\matr{x}; \theta_k),\quad \theta = (\pi, \theta_1, \ldots, \theta_K) \in \R^{K + K \cdot M}.
\]

\begin{itemize}
\item mixing proportions: \(\pi \geq 0,\ \sum_{k=1}^K \pi_k = 1\)
\item mixture components: \(p(\cdot;\theta_k)\) with \(\theta_k \in \R^M\)
\end{itemize}
\end{definition}

\subsection{Gaussian Mixture Model}
\begin{definition}
For the \textit{Gaussian Mixture Model (GMM)} the probability distribution is defined as
\[
p_\theta(\matr{x}) = \sum_{k=1}^K \pi_k \ \mathcal{N}(\matr{x}; \matr{\mu}_k, \matr{\Sigma}_k).
\]
\end{definition}

\todo{completely lost any form of understanding... lecture05 slide 9ff}