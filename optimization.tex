\chapter{Optimization}
The general optimization problem is to minimize a function \(f(\matr{x})\) with \(\matr{x} \in \R^D\). In this lecture the function is assumed to be \(f: \R^D \to \R\), continuous and differentiable. Therefore, we are interested in finding \(\matr{x}^\star \in \R^D\) such that \(f(\matr{x}^\star)\) is minimal. 

\section{Coordinate Descent}
The basic idea is to change only one variable while keeping all others fixed.

\begin{algorithm}[H]
\caption{Coordinate Descent}
\begin{algorithmic}[1]
\Procedure{coordinate\_descent}{}
\State initialize \(\matr{x}^{(0)} \in \R^D\)
\For{t = 0:maxIter}
	\State \(d \gets\) uniformly at random from \(1, \ldots, D\)
	\State \(u^\star \gets \argmin_{u \in \R} f(x_1^{(t)}, x_2^{(t)}, \ldots, x_{d-1}^{(t)}, u, x_{d+1}^{(t)}, \ldots, x_D^{(t)})\)
	\State \(\matr{x}_d^{(t+1)} \gets u^\star\) \Comment{use the newly found value for the \(d\)-th variable}
	\State \(\matr{x}_{d'}^{(t+1)} \gets \matr{x}_{d'}^{(t)} \; \forall d' \neq d\) \Comment{use all other variables as they were in the previous round}
\EndFor \Comment{the last ``version'' (\(\matr{x}^{(t+1)}\)) should lead to the smallest value of \(f\) we were able to find in maxIter iterations}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Gradient Descent}
A function (note the kind of functions we consider in the beginning of this chapter) \emph{decreases} in value fastest if one follows in \emph{opposite} direction of the gradient of that function at the current point. Going into the same direction as the gradient would lead us to a (local) maximum.

The \emph{gradient} of a function \(f: \R^D \to \R\) is
\[
\nabla f(\matr{x}) := \left( \frac{\partial f(\matr{x})}{\partial \matr{x}_1}, \ldots, \frac{\partial f(\matr{x})}{\partial \matr{x}_D} \right)^\top \in \R^D
\]

\begin{algorithm}[H]
\caption{Gradient Descent}
\begin{algorithmic}[1]
\Procedure{gradient\_descent}{}
\State initialize \(\matr{x}^{(0)}\)
\For{t = 0:maxIters}
	\State \(\matr{x}^{(t+1)} \gets \matr{x}^{(t)} - \gamma \nabla f(\matr{x}^{(t)})\)
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

The \emph{stepsize} \(\gamma\) is usually decreased in each iteration such that \(\gamma \approx \frac{1}{t}\).

\section{Stochastic Gradient Descent}
For function of the form
\[
f(\matr{x}) = \frac{1}{N} \sum_{n=1}^N f_n(\matr{x})
\]
the stochastic gradient descent can be applied.

\begin{algorithm}[H]
\caption{Stochastic Gradient Descent}
\begin{algorithmic}[1]
\Procedure{stochastic\_gradient\_descent}{}
\State initialize \(\matr{x}^{(0)} \in \R^D\)
\For{t = 0:maxIter}
	\State \(n \gets\) uniformly at random from \(1, \ldots, N\)
	\State \(\matr{x}^{(t+1)} \gets \matr{x}^{(t)} - \gamma \nabla f_n(\matr{x}^{(t)})\)
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Compared to the previous gradient descent this method is computationally cheaper, while keeping the estimate of the gradient unbiased (\( E[\nabla f_n(\matr{x})] = \nabla f(\matr{x}) \)) as long as we select \(n\) at random.

Here too the stepsize is usually decreased, i.e. \(\gamma \approx \frac{1}{t}\).

\section{Constrained Optimization}
In case of constrained optimization we are interested in minimizing \(f(\matr{x})\) as before, but for a \(\matr{x} \in Q \subseteq \R^D\), i.e. \(\matr{x}\) is expected to be part of a given set.

\subsection{Projected Gradient Descent}
Use gradient descent, but project the resulting \(\matr{x}^{(i)}\) into \(Q\). To project \(\matr{x}^{(i)}\) into \(Q\) the projection function
\[
P_Q(\matr{x}) := \argmin_{y \in Q} \|y - x\|
\]
is used.

\begin{algorithm}[H]
\caption{Projected Gradient Descent}
\begin{algorithmic}[1]
\Procedure{projected\_gradient\_descent}{}
\State initialize \(\matr{x}^{(0)}\)
\For{t = 0:maxIters}
	\State \(\matr{x}^{(t+1)} \gets P_Q\left(\matr{x}^{(t)} - \gamma \nabla f(\matr{x}^{(t)})\right)\)
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Constrained Optimization as Unconstrained}
A constrained optimization can be modified to be an unconstrained one using an \emph{penalty function}. Let \(P(\matr{x})\) be a penalty function. Now we can use unconstrained optimization algorithms to solve \(f(\matr{x})\) by optimizing \(f(\matr{x}) + P(\matr{x})\) instead.

There are multiple ways to define meaningful penalty function:
\begin{itemize}
\item Indicator function: \( P(\matr{x}) = I_Q(\matr{x}) := \begin{cases} 0 & x \in Q\\ \infty & x \neq Q \end{cases}\)
\item Penalize error, e.g. \(Q = \{\matr{x} \in \R^D \ |\ \matr{Ax} = \matr{b}\} \Rightarrow P(\matr{x}) = \lambda \|\matr{Ax} - \matr{b}\|^2\)
\item Linearized penalty functions
\end{itemize}

\section{Duality for Constrained Optimization}
We are interested in minimizing \(f(\matr{x})\) with subject to
\begin{align*}
g_i(\matr{x}) \leq 0,\, i = 1, \ldots, m\\
h_i(\matr{x}) = 0,\, i = 1, \ldots, p
\end{align*}
with \(f(\matr{x})\) being the \emph{objective function}, \(g_i(\matr{x})\) the \emph{inequality constraint functions}, and \(h_i(\matr{x})\) the \emph{affine equality constraint functions}. Further, \[h_i(\matr{x}) = \matr{a}_i^\top \matr{x} - b_i.\]

This constrained minimization problem can be translated into a unconstrained one:
\begin{eqnarray*}
&\min f(\matr{x}) + \sum_{i=1}^m I\_(g_i(\matr{x})) + \sum_{i=1}^p I_0(h_i(\matr{x}))\\
&\text{with}\\
&I\_(y) := \begin{cases} 0 & y \leq 0\\ \infty & y > 0 \end{cases} \quad\text{and}\quad
I_0(y) := \begin{cases} 0 & u = 0\\ \infty & u \neq 0 \end{cases}
\end{eqnarray*}

It is possible \todo{why?} to approximate \(I\_(y)\) and \(I_0(y)\) linearly with \(\lambda_i y\) and \(\nu_i y\) leading us to the \emph{Lagrangian}
\[
L(\matr{x}, \matr{\lambda}, \matr{\nu}) := f(\matr{x}) + \sum_{i = 1}^m \matr{\lambda}_i g_i(\matr{x}) + \sum_{i = 1}^p \matr{\nu}_i h_i(\matr{x}).
\]
\(\lambda_i\) and \(\nu_i\) are called \emph{Langrange multipliers}. The \emph{Lagrange dual function} is defined by
\[
d(\matr{\lambda}, \matr{\nu}) := \inf_{x} L(\matr{x}, \matr{\lambda}, \matr{\nu}) \; \in \R.
\]

Since \(\lambda_i u \leq I\_(u)\) and \(\nu_i u \leq I_0(u)\) for all \(u\) the dual function gives us always a lower bound on the primal value \(f(\matr{x})\) of any feasible \(\matr{x}\).

\begin{definition}[Lagrange dual problem]
\todo{I don't completely get this... try to understand it}
The Lagrange dual problem is the problem of maximizing the Lagrange dual function in regard to \(\matr{\lambda} \geq 0\). This gives us a lower bound on the unknown solution value of \(f(\matr{x}^\star)\).

In case the original optimization problem is \emph{convex}, and some additional conditions \todo{... way to go to with all this details in the slides ...}, the solution value of the dual problem is \emph{equal} to the solution value of \(f(\matr{x}^\star)\) of the primal problem!
\end{definition}

\section{Convex Optimization}
\begin{definition}[Convex Set]
A set \(Q\) is \emph{convex} if the line segment between two points \(\matr{x}, \matr{y} \in Q\) lies in \(Q\), i.e. \(\theta \matr{x} + (1-\theta) \matr{y} \in Q\).

Further:
\begin{itemize}
\item Intersection of convex sets are convex too
\item Projections onto convex sets are unique, and often efficient to compute (the \(P_Q\) function already discussed above)
\end{itemize}
\end{definition}

\begin{definition}[Convex Function]
A function \(f: \R^D \to \R\) is convex if
\begin{itemize}
\item \(\operatorname{dom}(f)\) is a convex set (this is the domain of the set, i.e. all ``valid input variables'' of the function), and
\item \(\forall \matr{x}, \matr{y} \in \operatorname{dom}(f), 0 \leq \theta \leq 1: f(\theta \matr{x} + (1-\theta) \matr{y}) \leq \theta f(\matr{x}) + (1-\theta) f(\matr{y})\), i.e. the line segment is ``above'' the graph of \(f\).
\end{itemize}
\end{definition}

\begin{definition}[Graph of a function]
The graph of a function \(f\) is defined by
\[
\{(\matr{x}, f(\matr{x}))\ |\ \matr{x} \in \operatorname{dom}(f)\}.
\]
\end{definition}

\begin{definition}[Epigraph of a function]
The epigraph of a function \(f\) is defined by
\[
\{(\matr{x}, t)\ |\ \matr{x} \in \operatorname{dom}(f), f(\matr{x}) \leq t\}.
\]

A function \(f\) is convex iff its epigraph is a convex set.
\end{definition}

Examples for convex functions:
\begin{itemize}
\item Linear functions: \(f(\matr{x}) = \matr{a}^\top \matr{x}\)
\item Affine functions : \(f(\matr{x}) = \matr{a}^\top \matr{x} + b\)
\item Exponential: \(f(\matr{x}) = e^{\alpha \matr{x}}\)
\item Every norm on \(\R^D\)
\end{itemize}

\todo{Exam: Somehow I feel like they might ask for a proof that norms are convex, as is shown on slide 36 in lecture07}

\emph{Convex optimization problems} are optimizations of a convex function \(f\) such that \(\matr{x}^\star \in Q\), with \(Q\) being a convex set. Note: \(\R^D\) is convex! All such convex optimization problems have the property that every local minimum is also a global minimum. All the previously shown algorithms in this chapter can be used for convex optimization problems and will \emph{converge} to the global optimum.

\section{SubGradient Descent}
\todo{not sure how/why it works}

 
